{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "Y = []\n",
    "\n",
    "pre_building_files = sorted(os.listdir('data/train_images/pre_buildings'))\n",
    "post_building_files = sorted(os.listdir('data/train_images/post_buildings'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_source = 'data/train_labels/post_labels/'\n",
    "post_hurricane_jsons = sorted([f for f in os.listdir(json_source)])\n",
    "\n",
    "damage_labels = []\n",
    "for image_json in post_hurricane_jsons:\n",
    "    with open (json_source + image_json) as f:\n",
    "        json_data = json.load(f)\n",
    "        \n",
    "    for building in json_data['features']['xy']:\n",
    "        damage_labels.append(building['properties']['subtype'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "Images = []\n",
    "for f in pre_building_files:\n",
    "    Images.append(Image.open('data/train_images/pre_buildings/' + f).convert('RGB'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pre = []\n",
    "for I in Images:\n",
    "    X_pre.append(np.asarray(I))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(66092, 50, 50, 3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_pre = np.array(X_pre)\n",
    "X_pre.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Images = []\n",
    "for f in post_building_files:\n",
    "    Images.append(Image.open('data/train_images/post_buildings/' + f).convert('RGB'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_post = []\n",
    "for I in Images:\n",
    "    X_post.append(np.asarray(I))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(66092, 50, 50, 3)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_post = np.array(X_post)\n",
    "X_post.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(66092, 50, 50, 6)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.concatenate((X_pre, X_post), axis=3)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66092"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(damage_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {'no-damage': 0, 'minor-damage': 1, 'major-damage': 2, 'destroyed': 3, 'un-classified': 4}\n",
    "\n",
    "for label in damage_labels:\n",
    "    Y.append(label_map[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52873\n",
      "13219\n",
      "52873\n",
      "13219\n"
     ]
    }
   ],
   "source": [
    "train_X = X.astype('float32')[:int(0.8*len(X))]\n",
    "test_X = X.astype('float32')[int(0.8*len(X)):]\n",
    "train_Y = Y[0:int(0.8*len(Y))]\n",
    "test_Y = Y[int(0.8*len(Y)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9446\n",
      "2373\n",
      "983\n",
      "303\n",
      "114\n"
     ]
    }
   ],
   "source": [
    "print(test_Y.count(0))\n",
    "print(test_Y.count(1))\n",
    "print(test_Y.count(2))\n",
    "print(test_Y.count(3))\n",
    "print(test_Y.count(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_X = X.astype('float32')\n",
    "# train_X /= 255\n",
    "\n",
    "# train_Y = Y\n",
    "\n",
    "# train_X.shape\n",
    "\n",
    "train_X /= 255\n",
    "test_X /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dropout, Flatten, Conv2D, MaxPooling2D, Dense, Activation\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.constraints import max_norm\n",
    "\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the network\n",
    "model.add(Conv2D(32, (3, 3), padding='same', kernel_initializer='glorot_normal', input_shape=(50, 50, 6)))\n",
    "#model.add(BatchNormalization(axis=-1))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(32, (3, 3), padding='same', kernel_initializer='glorot_normal'))\n",
    "#model.add(BatchNormalization(axis=-1))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Conv2D(64,(3, 3), padding='same', kernel_initializer='glorot_normal'))\n",
    "#model.add(BatchNormalization(axis=-1))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, (3, 3), padding='same', kernel_initializer='glorot_normal'))\n",
    "#model.add(BatchNormalization(axis=-1))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "# Fully connected layer\n",
    "model.add(Dense(512, kernel_initializer='glorot_normal'))\n",
    "#model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, kernel_initializer='glorot_normal'))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "\n",
    "# Defining loss and optimizer\n",
    "optimizer = RMSprop(lr=1e-4)\n",
    "loss = 'binary_crossentropy'\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 42298 samples, validate on 10575 samples\n",
      "Epoch 1/5\n",
      "42298/42298 [==============================] - 230s 5ms/step - loss: -9258.3329 - accuracy: 0.2133 - val_loss: 76643.4283 - val_accuracy: 0.3002\n",
      "Epoch 2/5\n",
      "42298/42298 [==============================] - 235s 6ms/step - loss: -537487.1157 - accuracy: 0.2129 - val_loss: 2420065.8081 - val_accuracy: 0.3002\n",
      "Epoch 3/5\n",
      "42298/42298 [==============================] - 235s 6ms/step - loss: -6681078.3614 - accuracy: 0.2129 - val_loss: 22243944.1101 - val_accuracy: 0.3002\n",
      "Epoch 4/5\n",
      "42298/42298 [==============================] - 262s 6ms/step - loss: -41960990.1185 - accuracy: 0.2129 - val_loss: 112196997.4271 - val_accuracy: 0.3002\n",
      "Epoch 5/5\n",
      "42298/42298 [==============================] - 265s 6ms/step - loss: -161281766.6061 - accuracy: 0.2129 - val_loss: 384462397.8818 - val_accuracy: 0.3002\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x883aff240>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 64\n",
    "EPOCHS = 5\n",
    "\n",
    "# test the network:\n",
    "model.fit(train_X, train_Y, batch_size=batch_size, epochs=EPOCHS, validation_split=0.2, verbose=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       ...,\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.]], dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loss, acc = model.evaluate(train_X, train_Y, batch_size=batch_size, verbose=1)\n",
    "\n",
    "# print('')\n",
    "# print('Got %.2f%% accuracy' % (acc * 100.))\n",
    "\n",
    "test_predict_Y = model.predict(test_X, batch_size=batch_size, verbose=0, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False)\n",
    "test_predict_Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of damaged buildings correctly labeled as damaged:  3773\n",
      "# of damaged buildings incorrectly labeled as undamaged:  0\n",
      "# of undamaged buildings correctly labeled as undamaged:  0\n",
      "# of undamaged buildings incorrectly labeled as damaged:  9446\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "correct_damaged = 0\n",
    "correct_undamaged = 0\n",
    "incorrect_damaged = 0\n",
    "incorrect_undamaged = 0\n",
    "for index in range(len(test_Y)):\n",
    "    if int(test_Y[index]) > 0:\n",
    "        if int(test_predict_Y[index]) > 0:\n",
    "            correct_damaged += 1\n",
    "        else:\n",
    "            incorrect_damaged += 1\n",
    "    else:\n",
    "        if int(test_predict_Y[index]) > 0:\n",
    "            incorrect_undamaged += 1\n",
    "        else:\n",
    "            correct_undamaged += 1\n",
    "\n",
    "print('# of damaged buildings correctly labeled as damaged: ', correct_damaged)\n",
    "print('# of damaged buildings incorrectly labeled as undamaged: ', incorrect_damaged)\n",
    "print('# of undamaged buildings correctly labeled as undamaged: ', correct_undamaged)\n",
    "print('# of undamaged buildings incorrectly labeled as damaged: ', incorrect_undamaged)\n",
    "    \n",
    "#multilabel_confusion_matrix(test_Y, test_predict_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       ...,\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.]], dtype=float32)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
